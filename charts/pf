---
# Source: orchestrator-operator/templates/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: sonataflow-infra
  annotations:
    argocd.argoproj.io/sync-wave: "-1"
---
# Source: orchestrator-operator/charts/helper-status-checker/templates/serviceAccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "orchestrator-operator-install-check"
  namespace: "openshift-operators"
  labels:
    helm.sh/chart: helper-status-checker-4.3.7
    app.kubernetes.io/name: helper-status-checker
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    argocd.argoproj.io/sync-wave: "0"
---
# Source: orchestrator-operator/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-postgresql
  namespace: "openshift-ingress-operator"
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 15.4.0
    helm.sh/chart: postgresql-12.12.10
type: Opaque
data:
  postgres-password: "SjNoamRyc0JBRw=="
  # We don't auto-generate LDAP password when it's not provided as we do for other passwords
---
# Source: orchestrator-operator/templates/secret-sonataflow-pgsql.yaml
kind: Secret
apiVersion: v1
metadata:
  name: sonataflow-psql-postgresql
  namespace: sonataflow-infra
stringData:
  postgres-password: postgres
  postgres-username: postgres
type: Opaque
---
# Source: orchestrator-operator/templates/greetings/01-configmap_greeting-props.yaml
apiVersion: v1
data:
  application.properties: |
    # This property is used to select the log level, which controls the amount
    # of information logged on HTTP requests based on the severity of the events.
    # Possible values: OFF, FATAL, ERROR, WARN, INFO, DEBUG, ALL.
    # and see https://quarkus.io/guides/logging for documentation
    quarkus.log.category."org.apache.http".level=INFO
    quarkus.log.level=INFO

    # quarkus.flyway.migrate-at-start=true
    kie.flyway.enabled = true
kind: ConfigMap
metadata:
  creationTimestamp: null
  labels:
    app: greeting
    sonataflow.org/workflow-app: greeting
  name: greeting-props
  namespace: sonataflow-infra
---
# Source: orchestrator-operator/templates/greetings/02-configmap_01-greeting-resources-schemas.yaml
apiVersion: v1
data:
  greeting.sw.input-schema.json: |
    {
      "$schema": "http://json-schema.org/draft-07/schema#",
      "type": "object",
      "properties": {
        "language": {
          "title": "Language",
          "description": "Language to greet",
          "type": "string",
          "enum": ["English", "Spanish"],
          "default": "English"
        }
      }
    }
  workflow-output-schema.json: |-
    {
        "$schema": "http://json-schema.org/draft-07/schema#",
        "title": "WorkflowResult",
        "description": "Schema of workflow output",
        "type": "object",
        "properties": {
            "result": {
                "$ref": "../shared/schemas/workflow-result-schema.json",
                "type": "object"
            }
        }
    }
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: 01-greeting-resources-schemas
  namespace: sonataflow-infra
---
# Source: orchestrator-operator/charts/helper-status-checker/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "0"
  name: orchestrator-operator-install-check-openshift-operators
  namespace: "openshift-operators"
  labels:
    helm.sh/chart: helper-status-checker-4.3.7
    app.kubernetes.io/name: helper-status-checker
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - operators.coreos.com
    resources:
      - clusterserviceversions
      - installplans
      - subscriptions
    verbs:
      - get
      - list
      - patch
---
# Source: orchestrator-operator/charts/helper-status-checker/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: orchestrator-operator-install-check-openshift-operators
  namespace: "openshift-operators"
  labels:
    helm.sh/chart: helper-status-checker-4.3.7
    app.kubernetes.io/name: helper-status-checker
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    argocd.argoproj.io/sync-wave: "0"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: orchestrator-operator-install-check-openshift-operators
subjects:
  - kind: ServiceAccount
    name: "orchestrator-operator-install-check"
    namespace: "openshift-operators"
---
# Source: orchestrator-operator/charts/postgresql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-postgresql-hl
  namespace: "openshift-ingress-operator"
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 15.4.0
    helm.sh/chart: postgresql-12.12.10
    app.kubernetes.io/component: primary
  annotations:
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/component: primary
---
# Source: orchestrator-operator/charts/postgresql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-postgresql
  namespace: "openshift-ingress-operator"
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 15.4.0
    helm.sh/chart: postgresql-12.12.10
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/component: primary
---
# Source: orchestrator-operator/charts/postgresql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-postgresql
  namespace: "openshift-ingress-operator"
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 15.4.0
    helm.sh/chart: postgresql-12.12.10
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  serviceName: release-name-postgresql-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: release-name-postgresql
      labels:
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: postgresql
        app.kubernetes.io/version: 15.4.0
        helm.sh/chart: postgresql-12.12.10
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/name: postgresql
                    app.kubernetes.io/component: primary
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      hostNetwork: false
      hostIPC: false
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:15.4.0-debian-11-r45
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-postgresql
                  key: postgres-password
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: orchestrator-operator/charts/helper-status-checker/templates/check_if_operator_is_ready.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: orchestrator-operator-install-check-check-operator-status
  namespace: "openshift-operators"
  annotations:
    argocd.argoproj.io/sync-wave: "0"
  labels:
    helm.sh/chart: helper-status-checker-4.3.7
    app.kubernetes.io/name: helper-status-checker
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  template:
    spec:
      containers:
        - image: registry.redhat.io/openshift4/ose-cli
          command:
            - /bin/bash
            - -c
            - |
             #!/usr/bin/env bash
             sleep_timer=5
             max_timer=120
             counter=0
             status=0

             echo "Starting operator status checker"
             operator=orchestrator-operator
             printf "\nWaiting for operator %s to appear\n" "$i";

             # Wait until operator appears
             SLEEPER_TMP=0
             get_exact_name=`oc get clusterserviceversion -n openshift-operators | grep $operator | awk -F" " '{print $1}'`
             until [ "$get_exact_name" != "" ]; do
               # do not wait forever
               let "counter=counter+1"
               echo "Attempt $counter of $max_timer"
               sleep 2
               if [[ $counter -eq $max_timer ]]
               then
                 echo "Giving up. Operator does not appear as clusterserviceversion"
                 exit 3
               fi

               echo "Operator not yet found... Give Operator $sleep_timer seconds to create the Deployment"
               while [[ $SLEEPER_TMP -le "$sleep_timer" ]]; do
                 echo -n "."
                 sleep 2
                 SLEEPER_TMP=$(($SLEEPER_TMP+1))
               done
               get_exact_name=`oc get clusterserviceversion -n openshift-operators | grep $operator | awk -F" " '{print $1}'`
             done

             # Get exact name of the operator
             counter=0
             SLEEPER_TMP=0
             get_status=`oc get clusterserviceversion -n openshift-operators $get_exact_name -o jsonpath={.status.phase}`
             until [ "$get_status" != "" ]; do
               # do not wait forever
               let "counter=counter+1"
               echo "Attempt $counter of $max_timer"

               if [[ $counter -eq $max_timer ]]
               then
                 echo "Giving up. Operator installation failed"
                 exit 3
               fi

               echo "Operator not yet ready ... Waiting $sleep_timer seconds"

               while [[ $SLEEPER_TMP -le "$sleep_timer" ]]; do
                 echo -n "."
                 sleep 1
                 SLEEPER_TMP=$(($SLEEPER_TMP+1))
               done
               get_status=`oc get clusterserviceversion -n openshift-operators $get_exact_name -o jsonpath={.status.phase}`
             done

             echo "Checking status of $get_exact_name. Current status: $get_status"

             # Wait until operator is ready
             counter=0
             SLEEPER_TMP=0
             until [ "$get_status" == "Succeeded" ]; do
             let "counter=counter+1"
               echo "Attempt $counter of $max_timer"

               if [[ $counter -eq $max_timer ]]
               then
                 echo "Giving up. Operator installation failed"
                 exit 3
               fi

               echo "Operator deployment is ongoing. Current status: $get_status"
               #sleep $sleep_timer

               while [[ $SLEEPER_TMP -le "$sleep_timer" ]]; do
                 echo -n "."
                 sleep 1
                 SLEEPER_TMP=$(($SLEEPER_TMP+1))
               done

               if [ "$get_status" == "Succeeded" ];
               then
                 status=$?
               fi
               get_status=`oc get clusterserviceversion -n openshift-operators $get_exact_name -o jsonpath={.status.phase}`
             done

             oc get clusterserviceversion -n openshift-operators $get_exact_name

             echo "Falling to sleep"

             SLEEPER_TMP=0

             while [[ $SLEEPER_TMP -le "10" ]]; do
                 echo -n "."
                 sleep 1
                 SLEEPER_TMP=$(($SLEEPER_TMP+1))
             done

             printf "\nStatus: $status\n"
             if [ $status -eq 0 ]; then
                  echo "Operator seems to be ready"
             else
                  echo "ERROR: Operator is not ready"
                  exit 1
             fi
          name: check-operator
      dnsPolicy: ClusterFirst
      restartPolicy: Never
      terminationGracePeriodSeconds: 30
      serviceAccountName: orchestrator-operator-install-check
---
# Source: orchestrator-operator/charts/helper-status-checker/templates/installplan-approver.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: orchestrator-operator-install-check-installplan-approver
  namespace: "openshift-operators"
  annotations:
    argocd.argoproj.io/sync-wave: "0"
  labels:
    helm.sh/chart: helper-status-checker-4.3.7
    app.kubernetes.io/name: helper-status-checker
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  backoffLimit: 3  # Retry up to 3 times if job fails
  template:
    spec:
      containers:
        - image: registry.redhat.io/openshift4/ose-cli
          command:
            - /bin/bash
            - -c
            - |
              export HOME=/tmp/approver

              echo "Approving operator install.  Waiting a few seconds to make sure the InstallPlan gets created first."
              sleep_timer=45

              SLEEPER_TMP=0
               while [[ $SLEEPER_TMP -le "$sleep_timer" ]]; do
                 echo -n "."
                 sleep 1
                 SLEEPER_TMP=$(($SLEEPER_TMP+1))
               done
              subscription=orchestrator-operator

              printf "\n\nProcessing subscription '$subscription'\n"

              # Retry logic for getting InstallPlan
              max_retries=10
              retry_count=0
              installplan=""
              
              while [ $retry_count -lt $max_retries ] && [ -z "$installplan" ]; do
                printf "\nAttempt $((retry_count + 1))/$max_retries: Getting InstallPlan for subscription '$subscription'\n"
                
                # Try to get the InstallPlan name
                installplan=$(oc get subscription ${subscription} -o jsonpath='{.status.installPlanRef.name}' 2>/dev/null || echo "")
                
                if [ -z "$installplan" ]; then
                  printf "No InstallPlan found yet, waiting 10 seconds before retry...\n"
                  sleep 10
                  retry_count=$((retry_count + 1))
                else
                  printf "Found InstallPlan: $installplan\n"
                  break
                fi
              done

              # Exit with error if no InstallPlan found after retries
              if [ -z "$installplan" ]; then
                printf "\nERROR: Could not find InstallPlan for subscription '$subscription' after $max_retries attempts\n"
                exit 1
              fi

              printf "\nCheck installplan approved status\n"

              # Check if InstallPlan exists and get approval status
              if ! oc get installplan $installplan >/dev/null 2>&1; then
                printf "\nERROR: InstallPlan '$installplan' not found\n"
                exit 1
              fi

              approved=$(oc get installplan $installplan -o jsonpath="{.spec.approved}" 2>/dev/null || echo "")
              
              if [ "$approved" == "false" ]; then
                printf "\nApproving Subscription $subscription with install plan $installplan\n"

                if oc patch installplan $installplan --type=json -p='[{"op":"replace","path": "/spec/approved", "value": true}]'; then
                  printf "\nSuccessfully approved InstallPlan '$installplan'\n"
                else
                  printf "\nERROR: Failed to approve InstallPlan '$installplan'\n"
                  exit 1
                fi
              else
                printf "\nInstall Plan '$installplan' already approved (status: $approved)\n"
              fi
          imagePullPolicy: Always
          name: installplan-approver
          env:
            - name: SLEEP
              value: "20"
      dnsPolicy: ClusterFirst
      restartPolicy: Never
      terminationGracePeriodSeconds: 30
      serviceAccountName: "orchestrator-operator-install-check"
---
# Source: orchestrator-operator/templates/orchestrator-rhdh.yaml
apiVersion: rhdh.redhat.com/v1alpha3
kind: Orchestrator
metadata:
  finalizers:
  - rhdh.redhat.com/orchestrator-cleanup
  generation: 1
  name: orchestrator-rhdh
  namespace: openshift-operators
  annotations:
    argocd.argoproj.io/sync-wave: "5"
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true,Validate=false
spec:
  argocd:
    enabled: false
  platform:
    eventing:
      broker: {}
    monitoring:
      enabled: false
    namespace: sonataflow-infra
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 250m
        memory: 64Mi
  postgres:
    authSecret:
      name: sonataflow-psql-postgresql
      passwordKey: postgres-password
      userKey: postgres-username
    database: sonataflow
    name: sonataflow-psql-postgresql
    namespace: sonataflow-infra
  rhdh:
    devMode: false
    installOperator: false
    name: backstage
    namespace: backstage
    plugins:
      notificationsEmail:
        enabled: false
        port: 587
        replyTo: ""
        sender: ""
  serverless:
    installOperator: true
  serverlessLogic:
    installOperator: true
  tekton:
    enabled: false
---
# Source: orchestrator-operator/templates/greetings/03-sonataflow_greeting.yaml
apiVersion: sonataflow.org/v1alpha08
kind: SonataFlow
metadata:
  annotations:
    sonataflow.org/description: YAML based greeting workflow
    sonataflow.org/expressionLang: jq
    sonataflow.org/profile: gitops
    sonataflow.org/version: "1.0"
  creationTimestamp: null
  labels:
    app: greeting
    sonataflow.org/workflow-app: greeting
  name: greeting
  namespace: sonataflow-infra
spec:
  flow:
    annotations:
      - workflow-type/infrastructure
    dataInputSchema:
      failOnValidationErrors: true
      schema: schemas/greeting.sw.input-schema.json
    functions:
      - name: greetFunction
        operation: sysout
        type: custom
      - name: successResult
        operation: '{ "result": { "message": "Greeting workflow completed successfully", "outputs":[ { "key":"Selected language", "value": .language }, { "key":"Greeting message", "value": .greeting } ] } }'
        type: expression
    start:
      stateName: ChooseOnLanguage
    states:
      - dataConditions:
          - condition: .language  == "English"
            transition:
              nextState: GreetInEnglish
          - condition: .language  == "Spanish"
            transition:
              nextState: GreetInSpanish
        defaultCondition:
          transition:
            nextState: GreetInEnglish
        name: ChooseOnLanguage
        type: switch
      - data:
          greeting: Hello from YAML Workflow
        name: GreetInEnglish
        transition:
          nextState: GreetPerson
        type: inject
      - data:
          greeting: Saludos desde YAML Workflow
        name: GreetInSpanish
        transition:
          nextState: GreetPerson
        type: inject
      - actionMode: sequential
        actions:
          - actionDataFilter:
              useResults: true
            functionRef:
              arguments:
                message: .greeting
              invoke: sync
              refName: greetFunction
            name: greetAction
          - actionDataFilter:
              useResults: true
            functionRef:
              invoke: sync
              refName: successResult
            name: setOutput
        end:
          terminate: true
        name: GreetPerson
        type: operation
  podTemplate:
    container:
      resources: {}
      image: quay.io/orchestrator/serverless-workflow-greeting:e783e6a7
  resources:
    configMaps:
      - configMap:
          name: 01-greeting-resources-schemas
        workflowPath: schemas
  persistence:
    postgresql:
      secretRef:
        name: sonataflow-psql-postgresql
        userKey: postgres-username
        passwordKey: postgres-password
      serviceRef:
        name: sonataflow-psql-postgresql
        port: 5432
        databaseName: sonataflow
        databaseSchema: greeting
---
# Source: orchestrator-operator/templates/subscription.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: orchestrator-operator
  namespace: openshift-operators
  annotations:
    argocd.argoproj.io/sync-wave: "2"
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
spec:
  channel: stable
  installPlanApproval: Manual
  name: orchestrator-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: orchestrator-operator.v1.6.1
