{{- if .Values.model.enabled }}
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: {{ .Values.model.name }}
  namespace: {{ .Values.namespace.name }}
  annotations:
    openshift.io/display-name: {{ .Values.model.displayName }}
    serving.kserve.io/deploymentMode: RawDeployment
    {{- if .Values.model.progressDeadlineSeconds }}
    serving.kserve.io/deploymentSpec: |
      {"progressDeadlineSeconds": {{ .Values.model.progressDeadlineSeconds }}}
    {{- end }}
    argocd.argoproj.io/sync-wave: "{{ .Values.model.syncwave }}"
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true,Validate=false
    argocd.argoproj.io/health: |
      hs = {}
      if obj.status ~= nil then
        if obj.status.conditions ~= nil then
          for i, condition in ipairs(obj.status.conditions) do
            if condition.type == "Ready" and condition.status == "True" then
              hs.status = "Healthy"
              hs.message = "InferenceService is ready"
              return hs
            elseif condition.type == "Ready" and condition.status == "False" then
              hs.status = "Degraded"
              hs.message = condition.message or "InferenceService not ready"
              return hs
            end
          end
        end
      end
      hs.status = "Progressing"
      hs.message = "Waiting for InferenceService to be ready"
      return hs
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  predictor:
    maxReplicas: {{ .Values.model.replicas.max }}
    minReplicas: {{ .Values.model.replicas.min }}
    model:
      modelFormat:
        name: vLLM
      name: ''
      resources:
        limits:
          cpu: {{ .Values.model.resources.limits.cpu | quote }}
          memory: {{ .Values.model.resources.limits.memory | quote }}
          nvidia.com/gpu: {{ .Values.model.resources.limits.gpu | quote }}
        requests:
          cpu: {{ .Values.model.resources.requests.cpu | quote }}
          memory: {{ .Values.model.resources.requests.memory | quote }}
          nvidia.com/gpu: {{ .Values.model.resources.requests.gpu | quote }}
      runtime: {{ .Values.servingRuntime.name }}
      storageUri: {{ .Values.model.storageUri }}
      args:
        {{- range .Values.model.args }}
        - {{ . | quote }}
        {{- end }}
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
{{- end }}

