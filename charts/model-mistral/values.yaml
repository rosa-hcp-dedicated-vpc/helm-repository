# Mistral Model Configuration
appTeam: model-mistral

# Namespace configuration
namespace:
  name: models
  create: true

# Sync wave for ArgoCD deployment
syncwave: 0

# vLLM ServingRuntime configuration
servingRuntime:
  enabled: true
  name: vllm-cuda-runtime
  displayName: "vLLM NVIDIA GPU ServingRuntime for KServe"
  image: "quay.io/modh/vllm:rhoai-2.20-cuda"
  # For AMD GPUs use: quay.io/modh/vllm:rhoai-2.20-rocm
  port: 8080
  syncwave: 0

# Model configuration
model:
  enabled: true
  name: mistral-small-24b-instruct-quantized
  displayName: "mistral-small-3-1-24b-instruct-2503-quantized-w4a16"
  storageUri: "oci://registry.redhat.io/rhelai1/modelcar-mistral-small-3-1-24b-instruct-2503-quantized-w4a16:1.5"
  syncwave: 1
  
  # Resource configuration
  resources:
    limits:
      cpu: "2"
      memory: "8Gi"
      gpu: "1"
    requests:
      cpu: "1"
      memory: "4Gi"
      gpu: "1"
  
  # Model arguments
  args:
    - "--max-model-len=8192"
    - "--gpu-memory-utilization=0.95"
  
  # Scaling configuration
  replicas:
    min: 1
    max: 1

# Route configuration
route:
  enabled: true
  tls:
    termination: edge

# Helper status checker dependency - checks serverless operator is ready
helper-status-checker:
  enabled: true
  approver: false
  checks:
    - operatorName: serverless-operator
      subscriptionName: serverless-operator
      namespace:
        name: openshift-serverless
      syncwave: 0
      serviceAccount:
        name: "mistral-serverless-check"
